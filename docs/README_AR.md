<h1 align="center">
    <a href="https://scrapling.readthedocs.io">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/D4Vinci/Scrapling/v4/docs/assets/cover_dark.svg?sanitize=true">
          <img alt="Scrapling Poster" src="https://raw.githubusercontent.com/D4Vinci/Scrapling/v4/docs/assets/cover_light.svg?sanitize=true">
        </picture>
    </a>
    <br>
    <small>Effortless Web Scraping for the Modern Web</small>
</h1>

<p align="center">
    <a href="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml" alt="Tests">
        <img alt="Tests" src="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg"></a>
    <a href="https://badge.fury.io/py/Scrapling" alt="PyPI version">
        <img alt="PyPI version" src="https://badge.fury.io/py/Scrapling.svg"></a>
    <a href="https://pepy.tech/project/scrapling" alt="PyPI Downloads">
        <img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/scrapling?period=total&units=INTERNATIONAL_SYSTEM&left_color=GREY&right_color=GREEN&left_text=Downloads"></a>
    <br/>
    <a href="https://discord.gg/EMgGbDceNQ" alt="Discord" target="_blank">
      <img alt="Discord" src="https://img.shields.io/discord/1360786381042880532?style=social&logo=discord&link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ">
    </a>
    <a href="https://x.com/Scrapling_dev" alt="X (formerly Twitter)">
      <img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/Scrapling_dev?style=social&logo=x&link=https%3A%2F%2Fx.com%2FScrapling_dev">
    </a>
    <br/>
    <a href="https://pypi.org/project/scrapling/" alt="Supported Python versions">
        <img alt="Supported Python versions" src="https://img.shields.io/pypi/pyversions/scrapling.svg"></a>
</p>

<p align="center">
    <a href="https://scrapling.readthedocs.io/en/latest/parsing/selection/"><strong>ุทุฑู ุงูุงุฎุชูุงุฑ</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/fetching/choosing/"><strong>ุงุฎุชูุงุฑ Fetcher</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/cli/overview/"><strong>ูุงุฌูุฉ ุณุทุฑ ุงูุฃูุงูุฑ</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/ai/mcp-server/"><strong>ูุถุน MCP</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/tutorials/migrating_from_beautifulsoup/"><strong>ุงูุงูุชูุงู ูู Beautifulsoup</strong></a>
</p>

Scrapling ูู ุฅุทุงุฑ ุนูู ุชูููู ูู Web Scraping ูุชุนุงูู ูุน ูู ุดูุก ูู ุทูุจ ูุงุญุฏ ุฅูู ุฒุญู ูุงูู ุงููุทุงู.

ูุญููู ูุชุนูู ูู ุชุบููุฑุงุช ุงูููุงูุน ููุนูุฏ ุชุญุฏูุฏ ูููุน ุนูุงุตุฑู ุชููุงุฆูุงู ุนูุฏ ุชุญุฏูุซ ุงูุตูุญุงุช. ุฌูุงูุจู ุชุชุฌุงูุฒ ุฃูุธูุฉ ููุงูุญุฉ ุงูุฑูุจูุชุงุช ูุซู Cloudflare Turnstile ูุจุงุดุฑุฉู. ูุฅุทุงุฑ ุนูู Spider ุงูุฎุงุต ุจู ูุชูุญ ูู ุงูุชูุณุน ุฅูู ุนูููุงุช ุฒุญู ูุชุฒุงููุฉ ููุชุนุฏุฏุฉ ุงูุฌูุณุงุช ูุน ุฅููุงู/ุงุณุชุฆูุงู ูุชุฏููุฑ ุชููุงุฆู ูู Proxy - ูู ุฐูู ูู ุจุถุนุฉ ุฃุณุทุฑ ูู Python. ููุชุจุฉ ูุงุญุฏุฉุ ุจุฏูู ุชูุงุฒูุงุช.

ุฒุญู ุณุฑูุน ููุบุงูุฉ ูุน ุฅุญุตุงุฆูุงุช ููุฑูุฉ ู Streaming. ูุจูู ุจูุงุณุทุฉ ูุณุชุฎุฑุฌู ุงูููุจ ููุณุชุฎุฑุฌู ุงูููุจ ูุงููุณุชุฎุฏููู ุงูุนุงุฏูููุ ููุงู ุดูุก ููุฌููุน.

```python
from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher
StealthyFetcher.adaptive = True
page = StealthyFetcher.fetch('https://example.com', headless=True, network_idle=True)  # ุงุญุตู ุนูู ุงููููุน ุจุดูู ุฎูู!
products = page.css('.product', auto_save=True)                                        # ุงุณุชุฎุฑุฌ ุจูุงูุงุช ุชูุฌู ูู ุชุบููุฑุงุช ุชุตููู ุงููููุน!
products = page.css('.product', adaptive=True)                                         # ูุงุญูุงูุ ุฅุฐุง ุชุบูุฑุช ุจููุฉ ุงููููุนุ ูุฑุฑ `adaptive=True` ููุนุซูุฑ ุนูููุง!
```
ุฃู ุชูุณุน ุฅูู ุนูููุงุช ุฒุญู ูุงููุฉ
```python
from scrapling.spiders import Spider, Response

class MySpider(Spider):
  name = "demo"
  start_urls = ["https://example.com/"]

  async def parse(self, response: Response):
      for item in response.css('.product'):
          yield {"title": item.css('h2::text').get()}

MySpider().start()
```


# ุงูุฑุนุงุฉ

<!-- sponsors -->

<a href="https://www.scrapeless.com/en?utm_source=official&utm_term=scrapling" target="_blank" title="Effortless Web Scraping Toolkit for Business and Developers"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/scrapeless.jpg"></a>
<a href="https://www.thordata.com/?ls=github&lk=github" target="_blank" title="Unblockable proxies and scraping infrastructure, delivering real-time, reliable web data to power AI models and workflows."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/thordata.jpg"></a>
<a href="https://evomi.com?utm_source=github&utm_medium=banner&utm_campaign=d4vinci-scrapling" target="_blank" title="Evomi is your Swiss Quality Proxy Provider, starting at $0.49/GB"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/evomi.png"></a>
<a href="https://serpapi.com/?utm_source=scrapling" target="_blank" title="Scrape Google and other search engines with SerpApi"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png"></a>
<a href="https://visit.decodo.com/Dy6W0b" target="_blank" title="Try the Most Efficient Residential Proxies for Free"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/decodo.png"></a>
<a href="https://petrosky.io/d4vinci" target="_blank" title="PetroSky delivers cutting-edge VPS hosting."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/petrosky.png"></a>
<a href="https://hasdata.com/?utm_source=github&utm_medium=banner&utm_campaign=D4Vinci" target="_blank" title="The web scraping service that actually beats anti-bot systems!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/hasdata.png"></a>
<a href="https://hypersolutions.co/?utm_source=github&utm_medium=readme&utm_campaign=scrapling" target="_blank" title="Bot Protection Bypass API for Akamai, DataDome, Incapsula & Kasada"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/HyperSolutions.png"></a>
<a href="https://www.swiftproxy.net/" target="_blank" title="Unlock Reliable Proxy Services with Swiftproxy!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.png"></a>
<a href="https://www.rapidproxy.io/?ref=d4v" target="_blank" title="Affordable Access to the Proxy World โ bypass CAPTCHAs blocks, and avoid additional costs."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/rapidproxy.jpg"></a>
<a href="https://browser.cash/?utm_source=D4Vinci&utm_medium=referral" target="_blank" title="Browser Automation & AI Browser Agent Platform"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/browserCash.png"></a>

<!-- /sponsors -->

<i><sub>ูู ุชุฑูุฏ ุนุฑุถ ุฅุนูุงูู ููุงุ ุงููุฑ [ููุง](https://github.com/sponsors/D4Vinci) ูุงุฎุชุฑ ุงููุณุชูู ุงูุฐู ููุงุณุจู!</sub></i>

---

## ุงูููุฒุงุช ุงูุฑุฆูุณูุฉ

### Spiders โ ุฅุทุงุฑ ุนูู ุฒุญู ูุงูู
- ๐ท๏ธ **ูุงุฌูุฉ Spider ุดุจููุฉ ุจู Scrapy**: ุนุฑูู Spiders ูุน `start_urls`ุ ู async `parse` callbacksุ ููุงุฆูุงุช `Request`/`Response`.
- โก **ุฒุญู ูุชุฒุงูู**: ุญุฏูุฏ ุชุฒุงูู ูุงุจูุฉ ููุชููููุ ูุชุญูู ุจุงูุณุฑุนุฉ ุญุณุจ ุงููุทุงูุ ูุชุฃุฎูุฑุงุช ุงูุชูุฒูู.
- ๐ **ุฏุนู ุงูุฌูุณุงุช ุงููุชุนุฏุฏุฉ**: ูุงุฌูุฉ ููุญุฏุฉ ูุทูุจุงุช HTTPุ ููุชุตูุญุงุช ุฎููุฉ ุจุฏูู ูุงุฌูุฉ ูู Spider ูุงุญุฏ โ ูุฌูู ุงูุทูุจุงุช ุฅูู ุฌูุณุงุช ูุฎุชููุฉ ุจุงููุนุฑูู.
- ๐พ **ุฅููุงู ูุงุณุชุฆูุงู**: ุงุณุชูุฑุงุฑูุฉ ุงูุฒุญู ุงููุงุฆูุฉ ุนูู Checkpoint. ุงุถุบุท Ctrl+C ููุฅููุงู ุจุณูุงุณุฉุ ุฃุนุฏ ุงูุชุดุบูู ููุงุณุชุฆูุงู ูู ุญูุซ ุชูููุช.
- ๐ก **ูุถุน Streaming**: ุจุซ ุงูุนูุงุตุฑ ุงููุณุชุฎุฑุฌุฉ ููุฑ ูุตูููุง ุนุจุฑ `async for item in spider.stream()` ูุน ุฅุญุตุงุฆูุงุช ููุฑูุฉ โ ูุซุงูู ููุงุฌูุงุช ุงููุณุชุฎุฏู ูุฎุทูุท ุงูุฃูุงุจูุจ ูุนูููุงุช ุงูุฒุญู ุงูุทูููุฉ.
- ๐ก๏ธ **ูุดู ุงูุทูุจุงุช ุงููุญุธูุฑุฉ**: ูุดู ุชููุงุฆู ูุฅุนุงุฏุฉ ูุญุงููุฉ ููุทูุจุงุช ุงููุญุธูุฑุฉ ูุน ููุทู ูุงุจู ููุชุฎุตูุต.
- ๐ฆ **ุชุตุฏูุฑ ูุฏูุฌ**: ุตุฏูุฑ ุงููุชุงุฆุฌ ุนุจุฑ ุงูุฎุทุงูุงุช ูุฎุท ุงูุฃูุงุจูุจ ุงูุฎุงุต ุจู ุฃู JSON/JSONL ุงููุฏูุฌ ูุน `result.items.to_json()` / `result.items.to_jsonl()` ุนูู ุงูุชูุงูู.

### ุฌูุจ ูุชูุฏู ููููุงูุน ูุน ุฏุนู ุงูุฌูุณุงุช
- **ุทูุจุงุช HTTP**: ุทูุจุงุช HTTP ุณุฑูุนุฉ ูุฎููุฉ ูุน ูุฆุฉ `Fetcher`. ูููููุง ุชูููุฏ ุจุตูุฉ TLS ูููุชุตูุญ ูุงูุฑุคูุณ ูุงุณุชุฎุฏุงู HTTP/3.
- **ุงูุชุญููู ุงูุฏููุงูููู**: ุฌูุจ ุงูููุงูุน ุงูุฏููุงููููุฉ ูุน ุฃุชูุชุฉ ูุงููุฉ ูููุชุตูุญ ูู ุฎูุงู ูุฆุฉ `DynamicFetcher` ุงูุชู ุชุฏุนู Chromium ูู Playwright ู Google Chrome.
- **ุชุฌุงูุฒ ููุงูุญุฉ ุงูุฑูุจูุชุงุช**: ูุฏุฑุงุช ุชุฎูู ูุชูุฏูุฉ ูุน `StealthyFetcher` ูุงูุชุญุงู fingerprint. ููููู ุชุฌุงูุฒ ุฌููุน ุฃููุงุน Turnstile/Interstitial ูู Cloudflare ุจุณูููุฉ ุจุงูุฃุชูุชุฉ.
- **ุฅุฏุงุฑุฉ ุงูุฌูุณุงุช**: ุฏุนู ุงูุฌูุณุงุช ุงููุณุชูุฑุฉ ูุน ูุฆุงุช `FetcherSession` ู`StealthySession` ู`DynamicSession` ูุฅุฏุงุฑุฉ ูููุงุช ุชุนุฑูู ุงูุงุฑุชุจุงุท ูุงูุญุงูุฉ ุนุจุฑ ุงูุทูุจุงุช.
- **ุชุฏููุฑ Proxy**: `ProxyRotator` ูุฏูุฌ ูุน ุงุณุชุฑุงุชูุฌูุงุช round-robin ุฃู ูุฎุตุตุฉ ุนุจุฑ ุฌููุน ุฃููุงุน ุงูุฌูุณุงุชุ ุจุงูุฅุถุงูุฉ ุฅูู ุชุฌุงูุฒุงุช Proxy ููู ุทูุจ.
- **ุญุธุฑ ุงููุทุงูุงุช**: ุญุธุฑ ุงูุทูุจุงุช ุฅูู ูุทุงูุงุช ูุญุฏุฏุฉ (ููุทุงูุงุชูุง ุงููุฑุนูุฉ) ูู ุงูุฌูุงูุจ ุงููุนุชูุฏุฉ ุนูู ุงููุชุตูุญ.
- **ุฏุนู Async**: ุฏุนู async ูุงูู ุนุจุฑ ุฌููุน ุงูุฌูุงูุจ ููุฆุงุช ุงูุฌูุณุงุช async ุงููุฎุตุตุฉ.

### ุงูุงุณุชุฎุฑุงุฌ ุงูุชูููู ูุงูุชูุงูู ูุน ุงูุฐูุงุก ุงูุงุตุทูุงุนู
- ๐ **ุชุชุจุน ุงูุนูุงุตุฑ ุงูุฐูู**: ุฅุนุงุฏุฉ ุชุญุฏูุฏ ูููุน ุงูุนูุงุตุฑ ุจุนุฏ ุชุบููุฑุงุช ุงููููุน ุจุงุณุชุฎุฏุงู ุฎูุงุฑุฒููุงุช ุงูุชุดุงุจู ุงูุฐููุฉ.
- ๐ฏ **ุงูุงุฎุชูุงุฑ ุงููุฑู ุงูุฐูู**: ูุญุฏุฏุงุช CSSุ ูุญุฏุฏุงุช XPathุ ุงูุจุญุซ ุงููุงุฆู ุนูู ุงูููุงุชุฑุ ุงูุจุญุซ ุงููุตูุ ุงูุจุญุซ ุจุงูุชุนุจูุฑุงุช ุงูุนุงุฏูุฉ ูุงููุฒูุฏ.
- ๐ **ุงูุจุญุซ ุนู ุนูุงุตุฑ ูุดุงุจูุฉ**: ุชุญุฏูุฏ ุงูุนูุงุตุฑ ุงููุดุงุจูุฉ ููุนูุงุตุฑ ุงูููุฌูุฏุฉ ุชููุงุฆูุงู.
- ๐ค **ุฎุงุฏู MCP ููุงุณุชุฎุฏุงู ูุน ุงูุฐูุงุก ุงูุงุตุทูุงุนู**: ุฎุงุฏู MCP ูุฏูุฌ ูู Web Scraping ุจูุณุงุนุฏุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช. ูุชููุฒ ุฎุงุฏู MCP ุจูุฏุฑุงุช ูููุฉ ูุฎุตุตุฉ ุชุณุชููุฏ ูู Scrapling ูุงุณุชุฎุฑุงุฌ ุงููุญุชูู ุงููุณุชูุฏู ูุจู ุชูุฑูุฑู ุฅูู ุงูุฐูุงุก ุงูุงุตุทูุงุนู (Claude/Cursor/ุฅูุฎ)ุ ูุจุงูุชุงูู ุชุณุฑูุน ุงูุนูููุงุช ูุชูููู ุงูุชูุงููู ุนู ุทุฑูู ุชูููู ุงุณุชุฎุฏุงู ุงูุฑููุฒ. ([ููุฏูู ุชูุถูุญู](https://www.youtube.com/watch?v=qyFk3ZNwOxE))

### ุจููุฉ ุนุงููุฉ ุงูุฃุฏุงุก ููุฎุชุจุฑุฉ ููุฏุงููุงู
- ๐ **ุณุฑูุน ูุงูุจุฑู**: ุฃุฏุงุก ูุญุณูู ูุชููู ุนูู ูุนุธู ููุชุจุงุช Web Scraping ูู Python.
- ๐ **ูุนุงู ูู ุงุณุชุฎุฏุงู ุงูุฐุงูุฑุฉ**: ููุงูู ุจูุงูุงุช ูุญุณููุฉ ูุชุญููู ูุณูู ูุฃูู ุงุณุชุฎุฏุงู ููุฐุงูุฑุฉ.
- โก **ุชุณูุณู JSON ุณุฑูุน**: ุฃุณุฑุน 10 ูุฑุงุช ูู ุงูููุชุจุฉ ุงูููุงุณูุฉ.
- ๐๏ธ **ููุฎุชุจุฑ ููุฏุงููุงู**: ูุง ููุชูู Scrapling ููุท ุชุบุทูุฉ ุงุฎุชุจุงุฑ ุจูุณุจุฉ 92ูช ูุชุบุทูุฉ ูุงููุฉ ูุชูููุญุงุช ุงูุฃููุงุนุ ุจู ุชู ุงุณุชุฎุฏุงูู ููููุงู ูู ูุจู ูุฆุงุช ูุณุชุฎุฑุฌู ุงูููุจ ุฎูุงู ุงูุนุงู ุงููุงุถู.

### ุชุฌุฑุจุฉ ุตุฏููุฉ ูููุทูุฑูู/ูุณุชุฎุฑุฌู ุงูููุจ
- ๐ฏ **Shell ุชูุงุนูู ูู Web Scraping**: Shell IPython ูุฏูุฌ ุงุฎุชูุงุฑู ูุน ุชูุงูู Scraplingุ ูุงุฎุชุตุงุฑุงุชุ ูุฃุฏูุงุช ุฌุฏูุฏุฉ ูุชุณุฑูุน ุชุทููุฑ ุณูุฑูุจุชุงุช Web Scrapingุ ูุซู ุชุญููู ุทูุจุงุช curl ุฅูู ุทูุจุงุช Scrapling ูุนุฑุถ ูุชุงุฆุฌ ุงูุทูุจุงุช ูู ูุชุตูุญู.
- ๐ **ุงุณุชุฎุฏูู ูุจุงุดุฑุฉ ูู ุงูุทุฑููุฉ**: ุงุฎุชูุงุฑูุงูุ ููููู ุงุณุชุฎุฏุงู Scrapling ูุงุณุชุฎุฑุงุฌ ุนููุงู URL ุฏูู ูุชุงุจุฉ ุณุทุฑ ูุงุญุฏ ูู ุงูููุฏ!
- ๐๏ธ **ูุงุฌูุฉ ุชููู ุบููุฉ**: ุงุฌุชูุงุฒ DOM ูุชูุฏู ูุน ุทุฑู ุงูุชููู ุจูู ุงูุนูุงุตุฑ ุงููุงูุฏูุฉ ูุงูุดูููุฉ ูุงููุฑุนูุฉ.
- ๐งฌ **ูุนุงูุฌุฉ ูุตูุต ูุญุณููุฉ**: ุชุนุจูุฑุงุช ุนุงุฏูุฉ ูุฏูุฌุฉ ูุทุฑู ุชูุธูู ูุนูููุงุช ูุตูุฉ ูุญุณููุฉ.
- ๐ **ุฅูุดุงุก ูุญุฏุฏุงุช ุชููุงุฆู**: ุฅูุดุงุก ูุญุฏุฏุงุช CSS/XPath ูููุฉ ูุฃู ุนูุตุฑ.
- ๐ **ูุงุฌูุฉ ูุฃูููุฉ**: ูุดุงุจู ูู Scrapy/BeautifulSoup ูุน ููุณ ุงูุนูุงุตุฑ ุงูุฒุงุฆูุฉ ุงููุณุชุฎุฏูุฉ ูู Scrapy/Parsel.
- ๐ **ุชุบุทูุฉ ูุงููุฉ ููุฃููุงุน**: ุชูููุญุงุช ููุน ูุงููุฉ ูุฏุนู IDE ููุชุงุฒ ูุฅููุงู ุงูููุฏ. ูุชู ูุญุต ูุงุนุฏุฉ ุงูููุฏ ุจุงููุงูู ุชููุงุฆูุงู ุจูุงุณุทุฉ **PyRight** ู**MyPy** ูุน ูู ุชุบููุฑ.
- ๐ **ุตูุฑุฉ Docker ุฌุงูุฒุฉ**: ูุน ูู ุฅุตุฏุงุฑุ ูุชู ุจูุงุก ูุฏูุน ุตูุฑุฉ Docker ุชุญุชูู ุนูู ุฌููุน ุงููุชุตูุญุงุช ุชููุงุฆูุงู.

## ุงูุจุฏุก

ููููู ูุธุฑุฉ ุณุฑูุนุฉ ุนูู ูุง ูููู ูู Scrapling ูุนูู ุฏูู ุงูุชุนูู.

### ุงูุงุณุชุฎุฏุงู ุงูุฃุณุงุณู
ุทูุจุงุช HTTP ูุน ุฏุนู ุงูุฌูุณุงุช
```python
from scrapling.fetchers import Fetcher, FetcherSession

with FetcherSession(impersonate='chrome') as session:  # ุงุณุชุฎุฏู ุฃุญุฏุซ ุฅุตุฏุงุฑ ูู ุจุตูุฉ TLS ูู Chrome
    page = session.get('https://quotes.toscrape.com/', stealthy_headers=True)
    quotes = page.css('.quote .text::text').getall()

# ุฃู ุงุณุชุฎุฏู ุทูุจุงุช ููุฑุฉ ูุงุญุฏุฉ
page = Fetcher.get('https://quotes.toscrape.com/')
quotes = page.css('.quote .text::text').getall()
```
ูุถุน ุงูุชุฎูู ุงููุชูุฏู
```python
from scrapling.fetchers import StealthyFetcher, StealthySession

with StealthySession(headless=True, solve_cloudflare=True) as session:  # ุฃุจูู ุงููุชุตูุญ ููุชูุญุงู ุญุชู ุชูุชูู
    page = session.fetch('https://nopecha.com/demo/cloudflare', google_search=False)
    data = page.css('#padded_content a').getall()

# ุฃู ุงุณุชุฎุฏู ููุท ุงูุทูุจ ููุฑุฉ ูุงุญุฏุฉุ ููุชุญ ุงููุชุตูุญ ููุฐุง ุงูุทูุจุ ุซู ูุบููู ุจุนุฏ ุงูุงูุชูุงุก
page = StealthyFetcher.fetch('https://nopecha.com/demo/cloudflare')
data = page.css('#padded_content a').getall()
```
ุฃุชูุชุฉ ุงููุชุตูุญ ุงููุงููุฉ
```python
from scrapling.fetchers import DynamicFetcher, DynamicSession

with DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # ุฃุจูู ุงููุชุตูุญ ููุชูุญุงู ุญุชู ุชูุชูู
    page = session.fetch('https://quotes.toscrape.com/', load_dom=False)
    data = page.xpath('//span[@class="text"]/text()').getall()  # ูุญุฏุฏ XPath ุฅุฐุง ููุช ุชูุถูู

# ุฃู ุงุณุชุฎุฏู ููุท ุงูุทูุจ ููุฑุฉ ูุงุญุฏุฉุ ููุชุญ ุงููุชุตูุญ ููุฐุง ุงูุทูุจุ ุซู ูุบููู ุจุนุฏ ุงูุงูุชูุงุก
page = DynamicFetcher.fetch('https://quotes.toscrape.com/')
data = page.css('.quote .text::text').getall()
```

### Spiders
ุงุจูู ุฒูุงุญู ูุงููุฉ ูุน ุทูุจุงุช ูุชุฒุงููุฉ ูุฃููุงุน ุฌูุณุงุช ูุชุนุฏุฏุฉ ูุฅููุงู/ุงุณุชุฆูุงู:
```python
from scrapling.spiders import Spider, Request, Response

class QuotesSpider(Spider):
    name = "quotes"
    start_urls = ["https://quotes.toscrape.com/"]
    concurrent_requests = 10

    async def parse(self, response: Response):
        for quote in response.css('.quote'):
            yield {
                "text": quote.css('.text::text').get(),
                "author": quote.css('.author::text').get(),
            }

        next_page = response.css('.next a')
        if next_page:
            yield response.follow(next_page[0].attrib['href'])

result = QuotesSpider().start()
print(f"Scraped {len(result.items)} quotes")
result.items.to_json("quotes.json")
```
ุงุณุชุฎุฏู ุฃููุงุน ุฌูุณุงุช ูุชุนุฏุฏุฉ ูู Spider ูุงุญุฏ:
```python
from scrapling.spiders import Spider, Request, Response
from scrapling.fetchers import FetcherSession, AsyncStealthySession

class MultiSessionSpider(Spider):
    name = "multi"
    start_urls = ["https://example.com/"]

    def configure_sessions(self, manager):
        manager.add("fast", FetcherSession(impersonate="chrome"))
        manager.add("stealth", AsyncStealthySession(headless=True), lazy=True)

    async def parse(self, response: Response):
        for link in response.css('a::attr(href)').getall():
            # ูุฌูู ุงูุตูุญุงุช ุงููุญููุฉ ุนุจุฑ ุฌูุณุฉ ุงูุชุฎูู
            if "protected" in link:
                yield Request(link, sid="stealth")
            else:
                yield Request(link, sid="fast", callback=self.parse)  # callback ุตุฑูุญ
```
ุฃููู ูุงุณุชุฃูู ุนูููุงุช ุงูุฒุญู ุงูุทูููุฉ ูุน Checkpoints ุจุชุดุบูู Spider ููุฐุง:
```python
QuotesSpider(crawldir="./crawl_data").start()
```
ุงุถุบุท Ctrl+C ููุฅููุงู ุจุณูุงุณุฉ โ ูุชู ุญูุธ ุงูุชูุฏู ุชููุงุฆูุงู. ูุงุญูุงูุ ุนูุฏ ุชุดุบูู Spider ูุฑุฉ ุฃุฎุฑูุ ูุฑุฑ ููุณ `crawldir`ุ ูุณูุณุชุฃูู ูู ุญูุซ ุชููู.

### ุงูุชุญููู ุงููุชูุฏู ูุงูุชููู
```python
from scrapling.fetchers import Fetcher

# ุงุฎุชูุงุฑ ุนูุงุตุฑ ุบูู ูุชููู
page = Fetcher.get('https://quotes.toscrape.com/')

# ุงุญุตู ุนูู ุงูุงูุชุจุงุณุงุช ุจุทุฑู ุงุฎุชูุงุฑ ูุชุนุฏุฏุฉ
quotes = page.css('.quote')  # ูุญุฏุฏ CSS
quotes = page.xpath('//div[@class="quote"]')  # XPath
quotes = page.find_all('div', {'class': 'quote'})  # ุจุฃุณููุจ BeautifulSoup
# ููุณ ุงูุดูุก ูุซู
quotes = page.find_all('div', class_='quote')
quotes = page.find_all(['div'], class_='quote')
quotes = page.find_all(class_='quote')  # ูููุฐุง...
# ุงูุจุญุซ ุนู ุนูุตุฑ ุจูุญุชูู ุงููุต
quotes = page.find_by_text('quote', tag='div')

# ุงูุชููู ุงููุชูุฏู
quote_text = page.css('.quote')[0].css('.text::text').get()
quote_text = page.css('.quote').css('.text::text').getall()  # ูุญุฏุฏุงุช ูุชุณูุณูุฉ
first_quote = page.css('.quote')[0]
author = first_quote.next_sibling.css('.author::text')
parent_container = first_quote.parent

# ุนูุงูุงุช ุงูุนูุงุตุฑ ูุงูุชุดุงุจู
similar_elements = first_quote.find_similar()
below_elements = first_quote.below_elements()
```
ููููู ุงุณุชุฎุฏุงู ุงููุญูู ูุจุงุดุฑุฉ ุฅุฐุง ููุช ูุง ุชุฑูุฏ ุฌูุจ ุงูููุงูุน ููุง ููู:
```python
from scrapling.parser import Selector

page = Selector("<html>...</html>")
```
ููู ูุนูู ุจููุณ ุงูุทุฑููุฉ ุชูุงูุงู!

### ุฃูุซูุฉ ุฅุฏุงุฑุฉ ุงูุฌูุณุงุช ุจุดูู Async
```python
import asyncio
from scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession

async with FetcherSession(http3=True) as session:  # `FetcherSession` ูุงุนู ุจุงูุณูุงู ููุนูู ูู ููุง ุงูููุทูู ุงููุชุฒุงูู/async
    page1 = session.get('https://quotes.toscrape.com/')
    page2 = session.get('https://quotes.toscrape.com/', impersonate='firefox135')

# ุงุณุชุฎุฏุงู ุฌูุณุฉ async
async with AsyncStealthySession(max_pages=2) as session:
    tasks = []
    urls = ['https://example.com/page1', 'https://example.com/page2']

    for url in urls:
        task = session.fetch(url)
        tasks.append(task)

    print(session.get_pool_stats())  # ุงุฎุชูุงุฑู - ุญุงูุฉ ูุฌููุนุฉ ุนูุงูุงุช ุชุจููุจ ุงููุชุตูุญ (ูุดุบูู/ุญุฑ/ุฎุทุฃ)
    results = await asyncio.gather(*tasks)
    print(session.get_pool_stats())
```

## ูุงุฌูุฉ ุณุทุฑ ุงูุฃูุงูุฑ ูุงูู Shell ุงูุชูุงุนูู

ูุชุถูู Scrapling ูุงุฌูุฉ ุณุทุฑ ุฃูุงูุฑ ูููุฉ:

[![asciicast](https://asciinema.org/a/736339.svg)](https://asciinema.org/a/736339)

ุชุดุบูู Shell ุงูู Web Scraping ุงูุชูุงุนูู
```bash
scrapling shell
```
ุงุณุชุฎุฑุฌ ุงูุตูุญุงุช ุฅูู ููู ูุจุงุดุฑุฉ ุฏูู ุจุฑูุฌุฉ (ูุณุชุฎุฑุฌ ุงููุญุชูู ุฏุงุฎู ูุณู `body` ุงูุชุฑุงุถูุงู). ุฅุฐุง ุงูุชูู ููู ุงูุฅุฎุฑุงุฌ ุจู `.txt`ุ ูุณูุชู ุงุณุชุฎุฑุงุฌ ูุญุชูู ุงููุต ูููุฏู. ุฅุฐุง ุงูุชูู ุจู `.md`ุ ูุณูููู ุชูุซูู Markdown ููุญุชูู HTMLุ ุฅุฐุง ุงูุชูู ุจู `.html`ุ ูุณูููู ูุญุชูู HTML ููุณู.
```bash
scrapling extract get 'https://example.com' content.md
scrapling extract get 'https://example.com' content.txt --css-selector '#fromSkipToProducts' --impersonate 'chrome'  # ุฌููุน ุงูุนูุงุตุฑ ุงููุทุงุจูุฉ ููุญุฏุฏ CSS '#fromSkipToProducts'
scrapling extract fetch 'https://example.com' content.md --css-selector '#fromSkipToProducts' --no-headless
scrapling extract stealthy-fetch 'https://nopecha.com/demo/cloudflare' captchas.html --css-selector '#padded_content a' --solve-cloudflare
```

> [!NOTE]
> ููุงู ุงูุนุฏูุฏ ูู ุงูููุฒุงุช ุงูุฅุถุงููุฉุ ููููุง ูุฑูุฏ ุฅุจูุงุก ูุฐู ุงูุตูุญุฉ ููุฌุฒุฉุ ุจูุง ูู ุฐูู ุฎุงุฏู MCP ูุงูู Shell ุงูุชูุงุนูู ูู Web Scraping. ุชุญูู ูู ุงููุซุงุฆู ุงููุงููุฉ [ููุง](https://scrapling.readthedocs.io/en/latest/)

## ูุนุงููุฑ ุงูุฃุฏุงุก

Scrapling ููุณ ูููุงู ูุญุณุจ โ ุจู ูู ุฃูุถุงู ุณุฑูุน ุจุดูู ูุฐูู. ุชูุงุฑู ุงููุนุงููุฑ ุงูุชุงููุฉ ูุญูู Scrapling ูุน ุฃุญุฏุซ ุฅุตุฏุงุฑุงุช ุงูููุชุจุงุช ุงูุดุงุฆุนุฉ ุงูุฃุฎุฑู.

### ุงุฎุชุจุงุฑ ุณุฑุนุฉ ุงุณุชุฎุฑุงุฌ ุงููุต (5000 ุนูุตุฑ ูุชุฏุงุฎู)

| # |      ุงูููุชุจุฉ      | ุงูููุช (ms) | vs Scrapling |
|---|:-----------------:|:----------:|:------------:|
| 1 |     Scrapling     |    2.02    |     1.0x     |
| 2 |   Parsel/Scrapy   |    2.04    |     1.01     |
| 3 |     Raw Lxml      |    2.54    |    1.257     |
| 4 |      PyQuery      |   24.17    |     ~12x     |
| 5 |    Selectolax     |   82.63    |     ~41x     |
| 6 |  MechanicalSoup   |  1549.71   |   ~767.1x    |
| 7 |   BS4 with Lxml   |  1584.31   |   ~784.3x    |
| 8 | BS4 with html5lib |  3391.91   |   ~1679.1x   |


### ุฃุฏุงุก ุชุดุงุจู ุงูุนูุงุตุฑ ูุงูุจุญุซ ุงููุตู

ูุฏุฑุงุช ุงูุนุซูุฑ ุนูู ุงูุนูุงุตุฑ ุงูุชููููุฉ ูู Scrapling ุชุชููู ุจุดูู ูุจูุฑ ุนูู ุงูุจุฏุงุฆู:

| ุงูููุชุจุฉ     | ุงูููุช (ms) | vs Scrapling |
|-------------|:----------:|:------------:|
| Scrapling   |    2.39    |     1.0x     |
| AutoScraper |   12.45    |    5.209x    |


> ุชูุซู ุฌููุน ุงููุนุงููุฑ ูุชูุณุทุงุช ุฃูุซุฑ ูู 100 ุชุดุบูู. ุงูุธุฑ [benchmarks.py](https://github.com/D4Vinci/Scrapling/blob/main/benchmarks.py) ูููููุฌูุฉ.

## ุงูุชุซุจูุช

ูุชุทูุจ Scrapling ุฅุตุฏุงุฑ Python 3.10 ุฃู ุฃุนูู:

```bash
pip install scrapling
```

ูุชุถูู ูุฐุง ุงูุชุซุจูุช ููุท ูุญุฑู ุงููุญูู ูุชุจุนูุงุชูุ ุจุฏูู ุฃู ุฌูุงูุจ ุฃู ุชุจุนูุงุช ุณุทุฑ ุงูุฃูุงูุฑ.

### ุงูุชุจุนูุงุช ุงูุงุฎุชูุงุฑูุฉ

1. ุฅุฐุง ููุช ุณุชุณุชุฎุฏู ุฃูุงู ูู ุงูููุฒุงุช ุงูุฅุถุงููุฉ ุฃุฏูุงูุ ุฃู ุงูุฌูุงูุจุ ุฃู ูุฆุงุชูุงุ ูุณุชุญุชุงุฌ ุฅูู ุชุซุจูุช ุชุจุนูุงุช ุงูุฌูุงูุจ ูุชุจุนูุงุช ุงููุชุตูุญ ุงูุฎุงุตุฉ ุจูุง ุนูู ุงููุญู ุงูุชุงูู:
    ```bash
    pip install "scrapling[fetchers]"

    scrapling install
    ```

    ูููู ูุฐุง ุจุชูุฒูู ุฌููุน ุงููุชุตูุญุงุชุ ุฅูู ุฌุงูุจ ุชุจุนูุงุช ุงููุธุงู ูุชุจุนูุงุช ูุนุงูุฌุฉ fingerprint ุงูุฎุงุตุฉ ุจูุง.

2. ููุฒุงุช ุฅุถุงููุฉ:
   - ุชุซุจูุช ููุฒุฉ ุฎุงุฏู MCP:
       ```bash
       pip install "scrapling[ai]"
       ```
   - ุชุซุจูุช ููุฒุงุช Shell (Shell ุงูู Web Scraping ูุฃูุฑ `extract`):
       ```bash
       pip install "scrapling[shell]"
       ```
   - ุชุซุจูุช ูู ุดูุก:
       ```bash
       pip install "scrapling[all]"
       ```
   ุชุฐูุฑ ุฃูู ุชุญุชุงุฌ ุฅูู ุชุซุจูุช ุชุจุนูุงุช ุงููุชุตูุญ ูุน `scrapling install` ุจุนุฏ ุฃู ูู ูุฐู ุงูุฅุถุงูุงุช (ุฅุฐุง ูู ุชูู ูุฏ ูุนูุช ุฐูู ุจุงููุนู)

### Docker
ููููู ุฃูุถุงู ุชุซุจูุช ุตูุฑุฉ Docker ูุน ุฌููุน ุงูุฅุถุงูุงุช ูุงููุชุตูุญุงุช ุจุงุณุชุฎุฏุงู ุงูุฃูุฑ ุงูุชุงูู ูู DockerHub:
```bash
docker pull pyd4vinci/scrapling
```
ุฃู ุชูุฒูููุง ูู ุณุฌู GitHub:
```bash
docker pull ghcr.io/d4vinci/scrapling:latest
```
ูุชู ุจูุงุก ูุฐู ุงูุตูุฑุฉ ูุฏูุนูุง ุชููุงุฆูุงู ุจุงุณุชุฎุฏุงู GitHub Actions ูุงููุฑุน ุงูุฑุฆูุณู ูููุณุชูุฏุน.

## ุงููุณุงููุฉ

ูุฑุญุจ ุจุงููุณุงููุงุช! ูุฑุฌู ูุฑุงุกุฉ [ุฅุฑุดุงุฏุงุช ุงููุณุงููุฉ](https://github.com/D4Vinci/Scrapling/blob/main/CONTRIBUTING.md) ูุจู ุงูุจุฏุก.

## ุฅุฎูุงุก ุงููุณุคูููุฉ

> [!CAUTION]
> ูุชู ุชูููุฑ ูุฐู ุงูููุชุจุฉ ููุฃุบุฑุงุถ ุงูุชุนููููุฉ ูุงูุจุญุซูุฉ ููุท. ุจุงุณุชุฎุฏุงู ูุฐู ุงูููุชุจุฉุ ูุฅูู ุชูุงูู ุนูู ุงูุงูุชุซุงู ูููุงููู ุงุณุชุฎุฑุงุฌ ุงูุจูุงูุงุช ูุงูุฎุตูุตูุฉ ุงููุญููุฉ ูุงูุฏูููุฉ. ุงููุคูููู ูุงููุณุงูููู ุบูุฑ ูุณุคูููู ุนู ุฃู ุฅุณุงุกุฉ ุงุณุชุฎุฏุงู ููุฐุง ุงูุจุฑูุงูุฌ. ุงุญุชุฑู ุฏุงุฆูุงู ุดุฑูุท ุฎุฏูุฉ ุงูููุงูุน ููููุงุช robots.txt.

## ุงูุชุฑุฎูุต

ูุฐุง ุงูุนูู ูุฑุฎุต ุจููุฌุจ ุชุฑุฎูุต BSD-3-Clause.

## ุงูุดูุฑ ูุงูุชูุฏูุฑ

ูุชุถูู ูุฐุง ุงููุดุฑูุน ููุฏุงู ูุนุฏูุงู ูู:
- Parsel (ุชุฑุฎูุต BSD) โ ููุณุชุฎุฏู ูููุญุฏุฉ ุงููุฑุนูุฉ [translator](https://github.com/D4Vinci/Scrapling/blob/main/scrapling/core/translator.py)

---
<div align="center"><small>ูุตูู ููุตููุน ุจู โค๏ธ ุจูุงุณุทุฉ ูุฑูู ุดุนูุฑ.</small></div><br>
