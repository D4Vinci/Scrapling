<h1 align="center">
    <a href="https://scrapling.readthedocs.io">
        <picture>
          <source media="(prefers-color-scheme: dark)" srcset="https://raw.githubusercontent.com/D4Vinci/Scrapling/dev/docs/assets/cover_dark.svg?sanitize=true">
          <img alt="Scrapling Poster" src="https://raw.githubusercontent.com/D4Vinci/Scrapling/dev/docs/assets/cover_light.svg?sanitize=true">
        </picture>
    </a>
    <br>
    <small>Effortless Web Scraping for the Modern Web</small>
</h1>

<p align="center">
    <a href="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml" alt="Tests">
        <img alt="Tests" src="https://github.com/D4Vinci/Scrapling/actions/workflows/tests.yml/badge.svg"></a>
    <a href="https://badge.fury.io/py/Scrapling" alt="PyPI version">
        <img alt="PyPI version" src="https://badge.fury.io/py/Scrapling.svg"></a>
    <a href="https://pepy.tech/project/scrapling" alt="PyPI Downloads">
        <img alt="PyPI Downloads" src="https://static.pepy.tech/personalized-badge/scrapling?period=total&units=INTERNATIONAL_SYSTEM&left_color=GREY&right_color=GREEN&left_text=Downloads"></a>
    <br/>
    <a href="https://discord.gg/EMgGbDceNQ" alt="Discord" target="_blank">
      <img alt="Discord" src="https://img.shields.io/discord/1360786381042880532?style=social&logo=discord&link=https%3A%2F%2Fdiscord.gg%2FEMgGbDceNQ">
    </a>
    <a href="https://x.com/Scrapling_dev" alt="X (formerly Twitter)">
      <img alt="X (formerly Twitter) Follow" src="https://img.shields.io/twitter/follow/Scrapling_dev?style=social&logo=x&link=https%3A%2F%2Fx.com%2FScrapling_dev">
    </a>
    <br/>
    <a href="https://pypi.org/project/scrapling/" alt="Supported Python versions">
        <img alt="Supported Python versions" src="https://img.shields.io/pypi/pyversions/scrapling.svg"></a>
</p>

<p align="center">
    <a href="https://scrapling.readthedocs.io/en/latest/parsing/selection/"><strong>é€‰æ‹©æ–¹æ³•</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/fetching/choosing/"><strong>é€‰æ‹©Fetcher</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/cli/overview/"><strong>CLI</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/ai/mcp-server/"><strong>MCPæ¨¡å¼</strong></a>
    &middot;
    <a href="https://scrapling.readthedocs.io/en/latest/tutorials/migrating_from_beautifulsoup/"><strong>ä»Beautifulsoupè¿ç§»</strong></a>
</p>

Scraplingæ˜¯ä¸€ä¸ªè‡ªé€‚åº”Web Scrapingæ¡†æ¶ï¼Œèƒ½å¤„ç†ä»å•ä¸ªè¯·æ±‚åˆ°å¤§è§„æ¨¡çˆ¬å–çš„ä¸€åˆ‡éœ€æ±‚ã€‚

å®ƒçš„è§£æå™¨èƒ½å¤Ÿä»ç½‘ç«™å˜åŒ–ä¸­å­¦ä¹ ï¼Œå¹¶åœ¨é¡µé¢æ›´æ–°æ—¶è‡ªåŠ¨é‡æ–°å®šä½æ‚¨çš„å…ƒç´ ã€‚å®ƒçš„Fetcherèƒ½å¤Ÿå¼€ç®±å³ç”¨åœ°ç»•è¿‡Cloudflare Turnstileç­‰åæœºå™¨äººç³»ç»Ÿã€‚å®ƒçš„Spideræ¡†æ¶è®©æ‚¨å¯ä»¥æ‰©å±•åˆ°å¹¶å‘ã€å¤šSessionçˆ¬å–ï¼Œæ”¯æŒæš‚åœ/æ¢å¤å’Œè‡ªåŠ¨Proxyè½®æ¢â€”â€”åªéœ€å‡ è¡ŒPythonä»£ç ã€‚ä¸€ä¸ªåº“ï¼Œé›¶å¦¥åã€‚

æé€Ÿçˆ¬å–ï¼Œå®æ—¶ç»Ÿè®¡å’ŒStreamingã€‚ç”±Web Scraperä¸ºWeb Scraperå’Œæ™®é€šç”¨æˆ·è€Œæ„å»ºï¼Œæ¯ä¸ªäººéƒ½èƒ½æ‰¾åˆ°é€‚åˆè‡ªå·±çš„åŠŸèƒ½ã€‚

```python
from scrapling.fetchers import Fetcher, AsyncFetcher, StealthyFetcher, DynamicFetcher
StealthyFetcher.adaptive = True
page = StealthyFetcher.fetch('https://example.com', headless=True, network_idle=True)  # éšç§˜åœ°è·å–ç½‘ç«™ï¼
products = page.css('.product', auto_save=True)                                        # æŠ“å–åœ¨ç½‘ç«™è®¾è®¡å˜æ›´åä»èƒ½å­˜æ´»çš„æ•°æ®ï¼
products = page.css('.product', adaptive=True)                                         # ä¹‹åï¼Œå¦‚æœç½‘ç«™ç»“æ„æ”¹å˜ï¼Œä¼ é€’ `adaptive=True` æ¥æ‰¾åˆ°å®ƒä»¬ï¼
```
æˆ–æ‰©å±•ä¸ºå®Œæ•´çˆ¬å–
```python
from scrapling.spiders import Spider, Response

class MySpider(Spider):
  name = "demo"
  start_urls = ["https://example.com/"]

  async def parse(self, response: Response):
      for item in response.css('.product'):
          yield {"title": item.css('h2::text').get()}

MySpider().start()
```


# èµåŠ©å•†

<!-- sponsors -->

<a href="https://www.scrapeless.com/en?utm_source=official&utm_term=scrapling" target="_blank" title="Effortless Web Scraping Toolkit for Business and Developers"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/scrapeless.jpg"></a>
<a href="https://www.thordata.com/?ls=github&lk=github" target="_blank" title="Unblockable proxies and scraping infrastructure, delivering real-time, reliable web data to power AI models and workflows."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/thordata.jpg"></a>
<a href="https://evomi.com?utm_source=github&utm_medium=banner&utm_campaign=d4vinci-scrapling" target="_blank" title="Evomi is your Swiss Quality Proxy Provider, starting at $0.49/GB"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/evomi.png"></a>
<a href="https://serpapi.com/?utm_source=scrapling" target="_blank" title="Scrape Google and other search engines with SerpApi"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/SerpApi.png"></a>
<a href="https://visit.decodo.com/Dy6W0b" target="_blank" title="Try the Most Efficient Residential Proxies for Free"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/decodo.png"></a>
<a href="https://petrosky.io/d4vinci" target="_blank" title="PetroSky delivers cutting-edge VPS hosting."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/petrosky.png"></a>
<a href="https://hasdata.com/?utm_source=github&utm_medium=banner&utm_campaign=D4Vinci" target="_blank" title="The web scraping service that actually beats anti-bot systems!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/hasdata.png"></a>
<a href="https://hypersolutions.co/?utm_source=github&utm_medium=readme&utm_campaign=scrapling" target="_blank" title="Bot Protection Bypass API for Akamai, DataDome, Incapsula & Kasada"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/HyperSolutions.png"></a>
<a href="https://www.swiftproxy.net/" target="_blank" title="Unlock Reliable Proxy Services with Swiftproxy!"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/swiftproxy.png"></a>
<a href="https://www.rapidproxy.io/?ref=d4v" target="_blank" title="Affordable Access to the Proxy World â€“ bypass CAPTCHAs blocks, and avoid additional costs."><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/rapidproxy.jpg"></a>
<a href="https://browser.cash/?utm_source=D4Vinci&utm_medium=referral" target="_blank" title="Browser Automation & AI Browser Agent Platform"><img src="https://raw.githubusercontent.com/D4Vinci/Scrapling/main/images/browserCash.png"></a>

<!-- /sponsors -->

<i><sub>æƒ³åœ¨è¿™é‡Œå±•ç¤ºæ‚¨çš„å¹¿å‘Šå—ï¼Ÿç‚¹å‡»[è¿™é‡Œ](https://github.com/sponsors/D4Vinci)å¹¶é€‰æ‹©é€‚åˆæ‚¨çš„çº§åˆ«ï¼</sub></i>

---

## ä¸»è¦ç‰¹æ€§

### Spider â€” å®Œæ•´çš„çˆ¬å–æ¡†æ¶
- ğŸ•·ï¸ **ç±»Scrapyçš„Spider API**ï¼šä½¿ç”¨`start_urls`ã€async `parse` callbackå’Œ`Request`/`Response`å¯¹è±¡å®šä¹‰Spiderã€‚
- âš¡ **å¹¶å‘çˆ¬å–**ï¼šå¯é…ç½®çš„å¹¶å‘é™åˆ¶ã€æŒ‰åŸŸåèŠ‚æµå’Œä¸‹è½½å»¶è¿Ÿã€‚
- ğŸ”„ **å¤šSessionæ”¯æŒ**ï¼šç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒHTTPè¯·æ±‚å’Œéšç§˜æ— å¤´æµè§ˆå™¨åœ¨åŒä¸€ä¸ªSpiderä¸­ä½¿ç”¨â€”â€”é€šè¿‡IDå°†è¯·æ±‚è·¯ç”±åˆ°ä¸åŒçš„Sessionã€‚
- ğŸ’¾ **æš‚åœä¸æ¢å¤**ï¼šåŸºäºCheckpointçš„çˆ¬å–æŒä¹…åŒ–ã€‚æŒ‰Ctrl+Cä¼˜é›…å…³é—­ï¼›é‡å¯åä»ä¸Šæ¬¡åœæ­¢çš„åœ°æ–¹ç»§ç»­ã€‚
- ğŸ“¡ **Streamingæ¨¡å¼**ï¼šé€šè¿‡`async for item in spider.stream()`ä»¥å®æ—¶ç»Ÿè®¡StreamingæŠ“å–çš„æ•°æ®â€”â€”éå¸¸é€‚åˆUIã€ç®¡é“å’Œé•¿æ—¶é—´è¿è¡Œçš„çˆ¬å–ã€‚
- ğŸ›¡ï¸ **è¢«é˜»æ­¢è¯·æ±‚æ£€æµ‹**ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶é‡è¯•è¢«é˜»æ­¢çš„è¯·æ±‚ï¼Œæ”¯æŒè‡ªå®šä¹‰é€»è¾‘ã€‚
- ğŸ“¦ **å†…ç½®å¯¼å‡º**ï¼šé€šè¿‡é’©å­å’Œæ‚¨è‡ªå·±çš„ç®¡é“å¯¼å‡ºç»“æœï¼Œæˆ–ä½¿ç”¨å†…ç½®çš„JSON/JSONLï¼Œåˆ†åˆ«é€šè¿‡`result.items.to_json()`/`result.items.to_jsonl()`ã€‚

### æ”¯æŒSessionçš„é«˜çº§ç½‘ç«™è·å–
- **HTTPè¯·æ±‚**ï¼šä½¿ç”¨`Fetcher`ç±»è¿›è¡Œå¿«é€Ÿå’Œéšç§˜çš„HTTPè¯·æ±‚ã€‚å¯ä»¥æ¨¡æ‹Ÿæµè§ˆå™¨çš„TLS fingerprintã€æ ‡å¤´å¹¶ä½¿ç”¨HTTP/3ã€‚
- **åŠ¨æ€åŠ è½½**ï¼šé€šè¿‡`DynamicFetcher`ç±»ä½¿ç”¨å®Œæ•´çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–è·å–åŠ¨æ€ç½‘ç«™ï¼Œæ”¯æŒPlaywrightçš„Chromiumå’ŒGoogle Chromeã€‚
- **åæœºå™¨äººç»•è¿‡**ï¼šä½¿ç”¨`StealthyFetcher`çš„é«˜çº§éšç§˜åŠŸèƒ½å’Œfingerprintä¼ªè£…ã€‚å¯ä»¥è½»æ¾è‡ªåŠ¨ç»•è¿‡æ‰€æœ‰ç±»å‹çš„Cloudflare Turnstile/Interstitialã€‚
- **Sessionç®¡ç†**ï¼šä½¿ç”¨`FetcherSession`ã€`StealthySession`å’Œ`DynamicSession`ç±»å®ç°æŒä¹…åŒ–Sessionæ”¯æŒï¼Œç”¨äºè·¨è¯·æ±‚çš„cookieå’ŒçŠ¶æ€ç®¡ç†ã€‚
- **Proxyè½®æ¢**ï¼šå†…ç½®`ProxyRotator`ï¼Œæ”¯æŒè½®è¯¢æˆ–è‡ªå®šä¹‰ç­–ç•¥ï¼Œé€‚ç”¨äºæ‰€æœ‰Sessionç±»å‹ï¼Œå¹¶æ”¯æŒæŒ‰è¯·æ±‚è¦†ç›–Proxyã€‚
- **åŸŸåå±è”½**ï¼šåœ¨åŸºäºæµè§ˆå™¨çš„Fetcherä¸­å±è”½å¯¹ç‰¹å®šåŸŸåï¼ˆåŠå…¶å­åŸŸåï¼‰çš„è¯·æ±‚ã€‚
- **Asyncæ”¯æŒ**ï¼šæ‰€æœ‰Fetcherå’Œä¸“ç”¨async Sessionç±»çš„å®Œæ•´asyncæ”¯æŒã€‚

### è‡ªé€‚åº”æŠ“å–å’ŒAIé›†æˆ
- ğŸ”„ **æ™ºèƒ½å…ƒç´ è·Ÿè¸ª**ï¼šä½¿ç”¨æ™ºèƒ½ç›¸ä¼¼æ€§ç®—æ³•åœ¨ç½‘ç«™æ›´æ”¹åé‡æ–°å®šä½å…ƒç´ ã€‚
- ğŸ¯ **æ™ºèƒ½çµæ´»é€‰æ‹©**ï¼šCSSé€‰æ‹©å™¨ã€XPathé€‰æ‹©å™¨ã€åŸºäºè¿‡æ»¤å™¨çš„æœç´¢ã€æ–‡æœ¬æœç´¢ã€æ­£åˆ™è¡¨è¾¾å¼æœç´¢ç­‰ã€‚
- ğŸ” **æŸ¥æ‰¾ç›¸ä¼¼å…ƒç´ **ï¼šè‡ªåŠ¨å®šä½ä¸å·²æ‰¾åˆ°å…ƒç´ ç›¸ä¼¼çš„å…ƒç´ ã€‚
- ğŸ¤– **ä¸AIä¸€èµ·ä½¿ç”¨çš„MCPæœåŠ¡å™¨**ï¼šå†…ç½®MCPæœåŠ¡å™¨ç”¨äºAIè¾…åŠ©Web Scrapingå’Œæ•°æ®æå–ã€‚MCPæœåŠ¡å™¨å…·æœ‰å¼ºå¤§çš„è‡ªå®šä¹‰åŠŸèƒ½ï¼Œåˆ©ç”¨Scraplingåœ¨å°†å†…å®¹ä¼ é€’ç»™AIï¼ˆClaude/Cursorç­‰ï¼‰ä¹‹å‰æå–ç›®æ ‡å†…å®¹ï¼Œä»è€ŒåŠ å¿«æ“ä½œå¹¶é€šè¿‡æœ€å°åŒ–tokenä½¿ç”¨æ¥é™ä½æˆæœ¬ã€‚ï¼ˆ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=qyFk3ZNwOxE)ï¼‰

### é«˜æ€§èƒ½å’Œç»è¿‡å®æˆ˜æµ‹è¯•çš„æ¶æ„
- ğŸš€ **é—ªç”µèˆ¬å¿«é€Ÿ**ï¼šä¼˜åŒ–æ€§èƒ½è¶…è¶Šå¤§å¤šæ•°PythonæŠ“å–åº“ã€‚
- ğŸ”‹ **å†…å­˜é«˜æ•ˆ**ï¼šä¼˜åŒ–çš„æ•°æ®ç»“æ„å’Œå»¶è¿ŸåŠ è½½ï¼Œæœ€å°å†…å­˜å ç”¨ã€‚
- âš¡ **å¿«é€ŸJSONåºåˆ—åŒ–**ï¼šæ¯”æ ‡å‡†åº“å¿«10å€ã€‚
- ğŸ—ï¸ **ç»è¿‡å®æˆ˜æµ‹è¯•**ï¼šScraplingä¸ä»…æ‹¥æœ‰92%çš„æµ‹è¯•è¦†ç›–ç‡å’Œå®Œæ•´çš„ç±»å‹æç¤ºè¦†ç›–ç‡ï¼Œè€Œä¸”åœ¨è¿‡å»ä¸€å¹´ä¸­æ¯å¤©è¢«æ•°ç™¾åWeb Scraperä½¿ç”¨ã€‚

### å¯¹å¼€å‘è€…/Web Scraperå‹å¥½çš„ä½“éªŒ
- ğŸ¯ **äº¤äº’å¼Web Scraping Shell**ï¼šå¯é€‰çš„å†…ç½®IPython Shellï¼Œå…·æœ‰Scraplingé›†æˆã€å¿«æ·æ–¹å¼å’Œæ–°å·¥å…·ï¼Œå¯åŠ å¿«Web Scrapingè„šæœ¬å¼€å‘ï¼Œä¾‹å¦‚å°†curlè¯·æ±‚è½¬æ¢ä¸ºScraplingè¯·æ±‚å¹¶åœ¨æµè§ˆå™¨ä¸­æŸ¥çœ‹è¯·æ±‚ç»“æœã€‚
- ğŸš€ **ç›´æ¥ä»ç»ˆç«¯ä½¿ç”¨**ï¼šå¯é€‰åœ°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ScraplingæŠ“å–URLè€Œæ— éœ€ç¼–å†™ä»»ä½•ä»£ç ï¼
- ğŸ› ï¸ **ä¸°å¯Œçš„å¯¼èˆªAPI**ï¼šä½¿ç”¨çˆ¶çº§ã€å…„å¼Ÿçº§å’Œå­çº§å¯¼èˆªæ–¹æ³•è¿›è¡Œé«˜çº§DOMéå†ã€‚
- ğŸ§¬ **å¢å¼ºçš„æ–‡æœ¬å¤„ç†**ï¼šå†…ç½®æ­£åˆ™è¡¨è¾¾å¼ã€æ¸…ç†æ–¹æ³•å’Œä¼˜åŒ–çš„å­—ç¬¦ä¸²æ“ä½œã€‚
- ğŸ“ **è‡ªåŠ¨é€‰æ‹©å™¨ç”Ÿæˆ**ï¼šä¸ºä»»ä½•å…ƒç´ ç”Ÿæˆå¼ºå¤§çš„CSS/XPathé€‰æ‹©å™¨ã€‚
- ğŸ”Œ **ç†Ÿæ‚‰çš„API**ï¼šç±»ä¼¼äºScrapy/BeautifulSoupï¼Œä½¿ç”¨ä¸Scrapy/Parselç›¸åŒçš„ä¼ªå…ƒç´ ã€‚
- ğŸ“˜ **å®Œæ•´çš„ç±»å‹è¦†ç›–**ï¼šå®Œæ•´çš„ç±»å‹æç¤ºï¼Œå‡ºè‰²çš„IDEæ”¯æŒå’Œä»£ç è¡¥å…¨ã€‚æ•´ä¸ªä»£ç åº“åœ¨æ¯æ¬¡æ›´æ”¹æ—¶éƒ½ä¼šè‡ªåŠ¨ä½¿ç”¨**PyRight**å’Œ**MyPy**æ‰«æã€‚
- ğŸ”‹ **ç°æˆçš„Dockeré•œåƒ**ï¼šæ¯æ¬¡å‘å¸ƒæ—¶ï¼ŒåŒ…å«æ‰€æœ‰æµè§ˆå™¨çš„Dockeré•œåƒä¼šè‡ªåŠ¨æ„å»ºå’Œæ¨é€ã€‚

## å…¥é—¨

è®©æˆ‘ä»¬å¿«é€Ÿå±•ç¤ºScraplingçš„åŠŸèƒ½ï¼Œæ— éœ€æ·±å…¥äº†è§£ã€‚

### åŸºæœ¬ç”¨æ³•
æ”¯æŒSessionçš„HTTPè¯·æ±‚
```python
from scrapling.fetchers import Fetcher, FetcherSession

with FetcherSession(impersonate='chrome') as session:  # ä½¿ç”¨Chromeçš„æœ€æ–°ç‰ˆæœ¬TLS fingerprint
    page = session.get('https://quotes.toscrape.com/', stealthy_headers=True)
    quotes = page.css('.quote .text::text').getall()

# æˆ–ä½¿ç”¨ä¸€æ¬¡æ€§è¯·æ±‚
page = Fetcher.get('https://quotes.toscrape.com/')
quotes = page.css('.quote .text::text').getall()
```
é«˜çº§éšç§˜æ¨¡å¼
```python
from scrapling.fetchers import StealthyFetcher, StealthySession

with StealthySession(headless=True, solve_cloudflare=True) as session:  # ä¿æŒæµè§ˆå™¨æ‰“å¼€ç›´åˆ°å®Œæˆ
    page = session.fetch('https://nopecha.com/demo/cloudflare', google_search=False)
    data = page.css('#padded_content a').getall()

# æˆ–ä½¿ç”¨ä¸€æ¬¡æ€§è¯·æ±‚æ ·å¼ï¼Œä¸ºæ­¤è¯·æ±‚æ‰“å¼€æµè§ˆå™¨ï¼Œå®Œæˆåå…³é—­
page = StealthyFetcher.fetch('https://nopecha.com/demo/cloudflare')
data = page.css('#padded_content a').getall()
```
å®Œæ•´çš„æµè§ˆå™¨è‡ªåŠ¨åŒ–
```python
from scrapling.fetchers import DynamicFetcher, DynamicSession

with DynamicSession(headless=True, disable_resources=False, network_idle=True) as session:  # ä¿æŒæµè§ˆå™¨æ‰“å¼€ç›´åˆ°å®Œæˆ
    page = session.fetch('https://quotes.toscrape.com/', load_dom=False)
    data = page.xpath('//span[@class="text"]/text()').getall()  # å¦‚æœæ‚¨åå¥½XPathé€‰æ‹©å™¨

# æˆ–ä½¿ç”¨ä¸€æ¬¡æ€§è¯·æ±‚æ ·å¼ï¼Œä¸ºæ­¤è¯·æ±‚æ‰“å¼€æµè§ˆå™¨ï¼Œå®Œæˆåå…³é—­
page = DynamicFetcher.fetch('https://quotes.toscrape.com/')
data = page.css('.quote .text::text').getall()
```

### Spider
æ„å»ºå…·æœ‰å¹¶å‘è¯·æ±‚ã€å¤šç§Sessionç±»å‹å’Œæš‚åœ/æ¢å¤åŠŸèƒ½çš„å®Œæ•´çˆ¬è™«ï¼š
```python
from scrapling.spiders import Spider, Request, Response

class QuotesSpider(Spider):
    name = "quotes"
    start_urls = ["https://quotes.toscrape.com/"]
    concurrent_requests = 10

    async def parse(self, response: Response):
        for quote in response.css('.quote'):
            yield {
                "text": quote.css('.text::text').get(),
                "author": quote.css('.author::text').get(),
            }

        next_page = response.css('.next a')
        if next_page:
            yield response.follow(next_page[0].attrib['href'])

result = QuotesSpider().start()
print(f"æŠ“å–äº† {len(result.items)} æ¡å¼•ç”¨")
result.items.to_json("quotes.json")
```
åœ¨å•ä¸ªSpiderä¸­ä½¿ç”¨å¤šç§Sessionç±»å‹ï¼š
```python
from scrapling.spiders import Spider, Request, Response
from scrapling.fetchers import FetcherSession, AsyncStealthySession

class MultiSessionSpider(Spider):
    name = "multi"
    start_urls = ["https://example.com/"]

    def configure_sessions(self, manager):
        manager.add("fast", FetcherSession(impersonate="chrome"))
        manager.add("stealth", AsyncStealthySession(headless=True), lazy=True)

    async def parse(self, response: Response):
        for link in response.css('a::attr(href)').getall():
            # å°†å—ä¿æŠ¤çš„é¡µé¢è·¯ç”±åˆ°éšç§˜Session
            if "protected" in link:
                yield Request(link, sid="stealth")
            else:
                yield Request(link, sid="fast", callback=self.parse)  # æ˜¾å¼callback
```
é€šè¿‡å¦‚ä¸‹æ–¹å¼è¿è¡ŒSpideræ¥æš‚åœå’Œæ¢å¤é•¿æ—¶é—´çˆ¬å–ï¼Œä½¿ç”¨Checkpointï¼š
```python
QuotesSpider(crawldir="./crawl_data").start()
```
æŒ‰Ctrl+Cä¼˜é›…æš‚åœâ€”â€”è¿›åº¦ä¼šè‡ªåŠ¨ä¿å­˜ã€‚ä¹‹åï¼Œå½“æ‚¨å†æ¬¡å¯åŠ¨Spideræ—¶ï¼Œä¼ é€’ç›¸åŒçš„`crawldir`ï¼Œå®ƒå°†ä»ä¸Šæ¬¡åœæ­¢çš„åœ°æ–¹ç»§ç»­ã€‚

### é«˜çº§è§£æä¸å¯¼èˆª
```python
from scrapling.fetchers import Fetcher

# ä¸°å¯Œçš„å…ƒç´ é€‰æ‹©å’Œå¯¼èˆª
page = Fetcher.get('https://quotes.toscrape.com/')

# ä½¿ç”¨å¤šç§é€‰æ‹©æ–¹æ³•è·å–å¼•ç”¨
quotes = page.css('.quote')  # CSSé€‰æ‹©å™¨
quotes = page.xpath('//div[@class="quote"]')  # XPath
quotes = page.find_all('div', {'class': 'quote'})  # BeautifulSoupé£æ ¼
# ç­‰åŒäº
quotes = page.find_all('div', class_='quote')
quotes = page.find_all(['div'], class_='quote')
quotes = page.find_all(class_='quote')  # ç­‰ç­‰...
# æŒ‰æ–‡æœ¬å†…å®¹æŸ¥æ‰¾å…ƒç´ 
quotes = page.find_by_text('quote', tag='div')

# é«˜çº§å¯¼èˆª
quote_text = page.css('.quote')[0].css('.text::text').get()
quote_text = page.css('.quote').css('.text::text').getall()  # é“¾å¼é€‰æ‹©å™¨
first_quote = page.css('.quote')[0]
author = first_quote.next_sibling.css('.author::text')
parent_container = first_quote.parent

# å…ƒç´ å…³ç³»å’Œç›¸ä¼¼æ€§
similar_elements = first_quote.find_similar()
below_elements = first_quote.below_elements()
```
å¦‚æœæ‚¨ä¸æƒ³è·å–ç½‘ç«™ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è§£æå™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
```python
from scrapling.parser import Selector

page = Selector("<html>...</html>")
```
ç”¨æ³•å®Œå…¨ç›¸åŒï¼

### Async Sessionç®¡ç†ç¤ºä¾‹
```python
import asyncio
from scrapling.fetchers import FetcherSession, AsyncStealthySession, AsyncDynamicSession

async with FetcherSession(http3=True) as session:  # `FetcherSession`æ˜¯ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ï¼Œå¯ä»¥åœ¨sync/asyncæ¨¡å¼ä¸‹å·¥ä½œ
    page1 = session.get('https://quotes.toscrape.com/')
    page2 = session.get('https://quotes.toscrape.com/', impersonate='firefox135')

# Async Sessionç”¨æ³•
async with AsyncStealthySession(max_pages=2) as session:
    tasks = []
    urls = ['https://example.com/page1', 'https://example.com/page2']

    for url in urls:
        task = session.fetch(url)
        tasks.append(task)

    print(session.get_pool_stats())  # å¯é€‰ - æµè§ˆå™¨æ ‡ç­¾æ± çš„çŠ¶æ€ï¼ˆå¿™/ç©ºé—²/é”™è¯¯ï¼‰
    results = await asyncio.gather(*tasks)
    print(session.get_pool_stats())
```

## CLIå’Œäº¤äº’å¼Shell

ScraplingåŒ…å«å¼ºå¤§çš„å‘½ä»¤è¡Œç•Œé¢ï¼š

[![asciicast](https://asciinema.org/a/736339.svg)](https://asciinema.org/a/736339)

å¯åŠ¨äº¤äº’å¼Web Scraping Shell
```bash
scrapling shell
```
ç›´æ¥å°†é¡µé¢æå–åˆ°æ–‡ä»¶è€Œæ— éœ€ç¼–ç¨‹ï¼ˆé»˜è®¤æå–`body`æ ‡ç­¾å†…çš„å†…å®¹ï¼‰ã€‚å¦‚æœè¾“å‡ºæ–‡ä»¶ä»¥`.txt`ç»“å°¾ï¼Œåˆ™å°†æå–ç›®æ ‡çš„æ–‡æœ¬å†…å®¹ã€‚å¦‚æœä»¥`.md`ç»“å°¾ï¼Œå®ƒå°†æ˜¯HTMLå†…å®¹çš„Markdownè¡¨ç¤ºï¼›å¦‚æœä»¥`.html`ç»“å°¾ï¼Œå®ƒå°†æ˜¯HTMLå†…å®¹æœ¬èº«ã€‚
```bash
scrapling extract get 'https://example.com' content.md
scrapling extract get 'https://example.com' content.txt --css-selector '#fromSkipToProducts' --impersonate 'chrome'  # æ‰€æœ‰åŒ¹é…CSSé€‰æ‹©å™¨'#fromSkipToProducts'çš„å…ƒç´ 
scrapling extract fetch 'https://example.com' content.md --css-selector '#fromSkipToProducts' --no-headless
scrapling extract stealthy-fetch 'https://nopecha.com/demo/cloudflare' captchas.html --css-selector '#padded_content a' --solve-cloudflare
```

> [!NOTE]
> è¿˜æœ‰è®¸å¤šå…¶ä»–åŠŸèƒ½ï¼Œä½†æˆ‘ä»¬å¸Œæœ›ä¿æŒæ­¤é¡µé¢ç®€æ´ï¼ŒåŒ…æ‹¬MCPæœåŠ¡å™¨å’Œäº¤äº’å¼Web Scraping Shellã€‚æŸ¥çœ‹å®Œæ•´æ–‡æ¡£[è¿™é‡Œ](https://scrapling.readthedocs.io/en/latest/)

## æ€§èƒ½åŸºå‡†

Scraplingä¸ä»…åŠŸèƒ½å¼ºå¤§â€”â€”å®ƒè¿˜é€Ÿåº¦æå¿«ã€‚ä»¥ä¸‹åŸºå‡†æµ‹è¯•å°†Scraplingçš„è§£æå™¨ä¸å…¶ä»–æµè¡Œåº“çš„æœ€æ–°ç‰ˆæœ¬è¿›è¡Œäº†æ¯”è¾ƒã€‚

### æ–‡æœ¬æå–é€Ÿåº¦æµ‹è¯•ï¼ˆ5000ä¸ªåµŒå¥—å…ƒç´ ï¼‰

| # |         åº“         | æ—¶é—´(ms)  | vs Scrapling |
|---|:-----------------:|:---------:|:------------:|
| 1 |     Scrapling     |   2.02    |     1.0x     |
| 2 |   Parsel/Scrapy   |   2.04    |     1.01     |
| 3 |     Raw Lxml      |   2.54    |    1.257     |
| 4 |      PyQuery      |   24.17   |     ~12x     |
| 5 |    Selectolax     |   82.63   |     ~41x     |
| 6 |  MechanicalSoup   |  1549.71  |   ~767.1x    |
| 7 |   BS4 with Lxml   |  1584.31  |   ~784.3x    |
| 8 | BS4 with html5lib |  3391.91  |   ~1679.1x   |


### å…ƒç´ ç›¸ä¼¼æ€§å’Œæ–‡æœ¬æœç´¢æ€§èƒ½

Scraplingçš„è‡ªé€‚åº”å…ƒç´ æŸ¥æ‰¾åŠŸèƒ½æ˜æ˜¾ä¼˜äºæ›¿ä»£æ–¹æ¡ˆï¼š

| åº“           | æ—¶é—´(ms) | vs Scrapling |
|-------------|:---------:|:------------:|
| Scrapling   |   2.39    |     1.0x     |
| AutoScraper |   12.45   |    5.209x    |


> æ‰€æœ‰åŸºå‡†æµ‹è¯•ä»£è¡¨100+æ¬¡è¿è¡Œçš„å¹³å‡å€¼ã€‚è¯·å‚é˜…[benchmarks.py](https://github.com/D4Vinci/Scrapling/blob/main/benchmarks.py)äº†è§£æ–¹æ³•ã€‚

## å®‰è£…

Scraplingéœ€è¦Python 3.10æˆ–æ›´é«˜ç‰ˆæœ¬ï¼š

```bash
pip install scrapling
```

æ­¤å®‰è£…ä»…åŒ…æ‹¬è§£æå™¨å¼•æ“åŠå…¶ä¾èµ–é¡¹ï¼Œæ²¡æœ‰ä»»ä½•Fetcheræˆ–å‘½ä»¤è¡Œä¾èµ–é¡¹ã€‚

### å¯é€‰ä¾èµ–é¡¹

1. å¦‚æœæ‚¨è¦ä½¿ç”¨ä»¥ä¸‹ä»»ä½•é¢å¤–åŠŸèƒ½ã€Fetcheræˆ–å®ƒä»¬çš„ç±»ï¼Œæ‚¨å°†éœ€è¦å®‰è£…Fetcherçš„ä¾èµ–é¡¹å’Œå®ƒä»¬çš„æµè§ˆå™¨ä¾èµ–é¡¹ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
    ```bash
    pip install "scrapling[fetchers]"

    scrapling install
    ```

    è¿™ä¼šä¸‹è½½æ‰€æœ‰æµè§ˆå™¨ï¼Œä»¥åŠå®ƒä»¬çš„ç³»ç»Ÿä¾èµ–é¡¹å’Œfingerprintæ“ä½œä¾èµ–é¡¹ã€‚

2. é¢å¤–åŠŸèƒ½ï¼š
   - å®‰è£…MCPæœåŠ¡å™¨åŠŸèƒ½ï¼š
       ```bash
       pip install "scrapling[ai]"
       ```
   - å®‰è£…ShellåŠŸèƒ½ï¼ˆWeb Scraping Shellå’Œ`extract`å‘½ä»¤ï¼‰ï¼š
       ```bash
       pip install "scrapling[shell]"
       ```
   - å®‰è£…æ‰€æœ‰å†…å®¹ï¼š
       ```bash
       pip install "scrapling[all]"
       ```
   è¯·è®°ä½ï¼Œåœ¨å®‰è£…ä»»ä½•è¿™äº›é¢å¤–åŠŸèƒ½åï¼ˆå¦‚æœæ‚¨è¿˜æ²¡æœ‰å®‰è£…ï¼‰ï¼Œæ‚¨éœ€è¦ä½¿ç”¨`scrapling install`å®‰è£…æµè§ˆå™¨ä¾èµ–é¡¹

### Docker
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä»DockerHubå®‰è£…åŒ…å«æ‰€æœ‰é¢å¤–åŠŸèƒ½å’Œæµè§ˆå™¨çš„Dockeré•œåƒï¼š
```bash
docker pull pyd4vinci/scrapling
```
æˆ–ä»GitHubæ³¨å†Œè¡¨ä¸‹è½½ï¼š
```bash
docker pull ghcr.io/d4vinci/scrapling:latest
```
æ­¤é•œåƒä½¿ç”¨GitHub Actionså’Œä»“åº“ä¸»åˆ†æ”¯è‡ªåŠ¨æ„å»ºå’Œæ¨é€ã€‚

## è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿è´¡çŒ®ï¼åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„[è´¡çŒ®æŒ‡å—](https://github.com/D4Vinci/Scrapling/blob/main/CONTRIBUTING.md)ã€‚

## å…è´£å£°æ˜

> [!CAUTION]
> æ­¤åº“ä»…ç”¨äºæ•™è‚²å’Œç ”ç©¶ç›®çš„ã€‚ä½¿ç”¨æ­¤åº“å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆæœ¬åœ°å’Œå›½é™…æ•°æ®æŠ“å–å’Œéšç§æ³•å¾‹ã€‚ä½œè€…å’Œè´¡çŒ®è€…å¯¹æœ¬è½¯ä»¶çš„ä»»ä½•æ»¥ç”¨ä¸æ‰¿æ‹…è´£ä»»ã€‚å§‹ç»ˆå°Šé‡ç½‘ç«™çš„æœåŠ¡æ¡æ¬¾å’Œrobots.txtæ–‡ä»¶ã€‚

## è®¸å¯è¯

æœ¬ä½œå“æ ¹æ®BSD-3-Clauseè®¸å¯è¯æˆæƒã€‚

## è‡´è°¢

æ­¤é¡¹ç›®åŒ…å«æ”¹ç¼–è‡ªä»¥ä¸‹å†…å®¹çš„ä»£ç ï¼š
- Parselï¼ˆBSDè®¸å¯è¯ï¼‰â€”â€”ç”¨äº[translator](https://github.com/D4Vinci/Scrapling/blob/main/scrapling/core/translator.py)å­æ¨¡å—

---
<div align="center"><small>ç”±Karim Shoairç”¨â¤ï¸è®¾è®¡å’Œåˆ¶ä½œã€‚</small></div><br>
